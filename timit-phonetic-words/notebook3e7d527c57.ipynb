{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 471627,
     "sourceType": "datasetVersion",
     "datasetId": 212391
    }
   ],
   "dockerImageVersionId": 30786,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "def get_tuples(filename):\n    words = []\n    with open(filename) as wf:\n        for line in wf.readlines():\n            line = line.strip()\n            parts = line.split(\" \")\n            if len(parts) != 3:\n                continue\n            words.append((int(parts[0]), int(parts[1]), parts[2]))\n    return words\n\ndef filter_junk(phones):\n    filtered = []\n    for phone in phones:\n        if phone[2].endswith(\"cl\"):\n            continue\n        if phone[2] in [\"epi\", \"pau\", \"h#\"]:\n            continue\n        if phone[2] == \"ax-h\":\n            filtered.append((phone[0], phone[1], \"ax\"))\n        else:\n            filtered.append(phone)\n    return filtered\n\ndef get_phonetic_words(filename):\n    if filename.endswith(\".WRD\"):\n        wordfile = filename\n        phonfile = wordfile.replace(\".WRD\", \".PHN\")\n    elif filename.endswith(\".PHN\"):\n        phonfile = filename\n        wordfile = phonfile.replace(\".PHN\", \".WRD\")\n    else:\n        return None\n    \n    words = get_tuples(wordfile)\n    phones = get_tuples(phonfile)\n    phones = filter_junk(phones)\n\n    def in_word(phone, word):\n        return (phone[0] >= word[0]) and (phone[1] <= word[1])\n    \n    merged = []\n    \n    i = j = 0\n    while i < len(words):\n        word = words[i]\n        current = {\n            \"start\": word[0],\n            \"end\": word[1],\n            \"word\": word[2],\n            \"phones\": []\n        }\n        while j < len(phones):\n            phone = phones[j]\n            if in_word(phone, word):\n                current[\"phones\"].append(phone[2])\n                j += 1\n            elif phone[0] >= word[1]:\n                # Phone starts at or after word end - move to next word\n                break\n            else:\n                # Phone starts before word but doesn't fit - skip it\n                j += 1\n        merged.append(current)\n        i += 1\n\n    return merged",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-03T21:09:51.513888Z",
     "iopub.status.idle": "2024-11-03T21:09:51.514309Z",
     "shell.execute_reply.started": "2024-11-03T21:09:51.514108Z",
     "shell.execute_reply": "2024-11-03T21:09:51.514129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Test the fixed function\nresult = get_phonetic_words(\"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/data/TRAIN/DR1/FCJF0/SA1.WRD\")\nfor item in result:\n    print(f\"{item['word']}: {' '.join(item['phones'])}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-03T21:09:51.826457Z",
     "iopub.execute_input": "2024-11-03T21:09:51.826880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import glob\nfrom collections import defaultdict\n\n# Collect all pronunciations from the corpus\nBASE_PATH = \"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/data\"\nword_pronunciations = defaultdict(list)\n\nfor wrd_file in glob.glob(f\"{BASE_PATH}/**/*.WRD\", recursive=True):\n    phonetic_words = get_phonetic_words(wrd_file)\n    if phonetic_words:\n        for item in phonetic_words:\n            word = item[\"word\"].lower()\n            phones = tuple(item[\"phones\"])\n            word_pronunciations[word].append(phones)\n\nprint(f\"Collected pronunciations for {len(word_pronunciations)} unique words\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load TIMIT dictionary\ndef load_timit_dict(filename):\n    \"\"\"Parse TIMITDIC.TXT format: word /pronunciation/\"\"\"\n    timit_dict = {}\n    with open(filename) as f:\n        for line in f:\n            line = line.strip()\n            if not line or line.startswith(\";\"):\n                continue\n            # Format: word /p r o n u n c i a t i o n/\n            if \"/\" not in line:\n                continue\n            word_part, pron_part = line.split(\"/\", 1)\n            word = word_part.strip().lower()\n            pron = tuple(pron_part.rstrip(\"/\").strip().split())\n            timit_dict[word] = pron\n    return timit_dict\n\n# Try common locations for the dictionary\ndict_paths = [\n    f\"{BASE_PATH}/TIMITDIC.TXT\",\n    f\"{BASE_PATH}/../TIMITDIC.TXT\",\n    \"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/TIMITDIC.TXT\",\n]\n\ntimit_dict = None\nfor path in dict_paths:\n    try:\n        timit_dict = load_timit_dict(path)\n        print(f\"Loaded dictionary from {path}: {len(timit_dict)} entries\")\n        break\n    except FileNotFoundError:\n        continue\n\nif timit_dict is None:\n    print(\"TIMIT dictionary not found - listing available files...\")\n    import os\n    for item in os.listdir(\"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/\"):\n        print(f\"  {item}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def align_sequences(ref, hyp):\n    \"\"\"\n    Align two phone sequences using dynamic programming.\n    Returns list of operations: ('match', r, h), ('sub', r, h), ('del', r, None), ('ins', None, h)\n    \"\"\"\n    m, n = len(ref), len(hyp)\n    \n    # DP table\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    # Initialize\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    \n    # Fill table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if ref[i-1] == hyp[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = 1 + min(\n                    dp[i-1][j],      # deletion\n                    dp[i][j-1],      # insertion\n                    dp[i-1][j-1]     # substitution\n                )\n    \n    # Backtrace\n    ops = []\n    i, j = m, n\n    while i > 0 or j > 0:\n        if i > 0 and j > 0 and ref[i-1] == hyp[j-1]:\n            ops.append(('match', ref[i-1], hyp[j-1]))\n            i -= 1\n            j -= 1\n        elif i > 0 and j > 0 and dp[i][j] == dp[i-1][j-1] + 1:\n            ops.append(('sub', ref[i-1], hyp[j-1]))\n            i -= 1\n            j -= 1\n        elif i > 0 and dp[i][j] == dp[i-1][j] + 1:\n            ops.append(('del', ref[i-1], None))\n            i -= 1\n        else:\n            ops.append(('ins', None, hyp[j-1]))\n            j -= 1\n    \n    return list(reversed(ops))\n\n# Test alignment\nref = ('dh', 'ax', 's', 't', 'ao', 'r')\nhyp = ('dh', 'ix', 's', 'ao', 'r')\nprint(\"Reference:\", ref)\nprint(\"Hypothesis:\", hyp)\nprint(\"Alignment:\")\nfor op in align_sequences(ref, hyp):\n    print(f\"  {op}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extract transformations across the corpus\nfrom collections import Counter\n\nsubstitutions = Counter()  # (ref_phone, actual_phone) -> count\ndeletions = Counter()      # ref_phone -> count\ninsertions = Counter()     # actual_phone -> count\nmatches = Counter()        # phone -> count (for computing rates)\n\nwords_analyzed = 0\nwords_not_in_dict = 0\n\nif timit_dict:\n    for word, pronunciations in word_pronunciations.items():\n        if word not in timit_dict:\n            words_not_in_dict += 1\n            continue\n        \n        dict_pron = timit_dict[word]\n        \n        for actual_pron in pronunciations:\n            words_analyzed += 1\n            alignment = align_sequences(dict_pron, actual_pron)\n            \n            for op, ref_phone, actual_phone in alignment:\n                if op == 'match':\n                    matches[ref_phone] += 1\n                elif op == 'sub':\n                    substitutions[(ref_phone, actual_phone)] += 1\n                elif op == 'del':\n                    deletions[ref_phone] += 1\n                elif op == 'ins':\n                    insertions[actual_phone] += 1\n\n    print(f\"Words analyzed: {words_analyzed}\")\n    print(f\"Words not in dictionary: {words_not_in_dict}\")\n    print(f\"\\nUnique substitution types: {len(substitutions)}\")\n    print(f\"Unique deletions: {len(deletions)}\")\n    print(f\"Unique insertions: {len(insertions)}\")\nelse:\n    print(\"Cannot analyze - dictionary not loaded\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define phonetic feature classes for context-sensitive rules\nVOWELS = {'aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ay', 'eh', 'er', 'ey', \n          'ih', 'ix', 'iy', 'ow', 'oy', 'uh', 'uw', 'ux', 'axr'}\nCONSONANTS = {'b', 'ch', 'd', 'dh', 'dx', 'f', 'g', 'hh', 'hv', 'jh', \n              'k', 'l', 'm', 'n', 'ng', 'nx', 'p', 'r', 's', 'sh', \n              't', 'th', 'v', 'w', 'y', 'z', 'zh', 'q'}\nSTOPS = {'b', 'p', 'd', 't', 'g', 'k', 'dx', 'q'}\nFRICATIVES = {'f', 'v', 'th', 'dh', 's', 'z', 'sh', 'zh', 'hh', 'hv'}\nNASALS = {'m', 'n', 'ng', 'nx', 'em', 'en', 'eng'}\nLIQUIDS = {'l', 'r', 'el'}\nGLIDES = {'w', 'y'}\nSYLLABICS = {'em', 'en', 'eng', 'el', 'axr'}\n\ndef get_phone_class(phone):\n    \"\"\"Return a set of feature classes for a phone\"\"\"\n    classes = set()\n    if phone in VOWELS:\n        classes.add('V')\n    if phone in CONSONANTS:\n        classes.add('C')\n    if phone in STOPS:\n        classes.add('stop')\n    if phone in FRICATIVES:\n        classes.add('fric')\n    if phone in NASALS:\n        classes.add('nasal')\n    if phone in LIQUIDS:\n        classes.add('liquid')\n    if phone in GLIDES:\n        classes.add('glide')\n    if phone in SYLLABICS:\n        classes.add('syllabic')\n    return classes\n\ndef get_context(phones, idx):\n    \"\"\"\n    Get the phonetic context for a phone at position idx.\n    Returns (left_context, right_context) as feature sets.\n    \"\"\"\n    left = get_phone_class(phones[idx-1]) if idx > 0 else {'#'}  # word boundary\n    right = get_phone_class(phones[idx+1]) if idx < len(phones)-1 else {'#'}\n    return (left, right)\n\ndef context_to_str(left, right):\n    \"\"\"Convert context to a readable string like 'V_V' or 'C_#'\"\"\"\n    l = 'V' if 'V' in left else ('C' if 'C' in left else '#')\n    r = 'V' if 'V' in right else ('C' if 'C' in right else '#')\n    return f\"{l}_{r}\"\n\nprint(\"Phone classes defined:\")\nprint(f\"  Vowels: {len(VOWELS)}\")\nprint(f\"  Consonants: {len(CONSONANTS)}\")\nprint(f\"  Syllabics: {SYLLABICS}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# TIMIT dialect regions\nDIALECT_REGIONS = {\n    'DR1': 'New England',\n    'DR2': 'Northern',\n    'DR3': 'North Midland', \n    'DR4': 'South Midland',\n    'DR5': 'Southern',\n    'DR6': 'New York City',\n    'DR7': 'Western',\n    'DR8': 'Army Brat (moved around)',\n}\n\ndef get_dialect_from_path(filepath):\n    \"\"\"Extract dialect region from TIMIT file path\"\"\"\n    import re\n    match = re.search(r'/(DR\\d)/', filepath)\n    if match:\n        return match.group(1)\n    return None\n\n# Re-collect pronunciations with file path info for dialect tracking\nword_pronunciations_detailed = defaultdict(list)\n\nfor wrd_file in glob.glob(f\"{BASE_PATH}/**/*.WRD\", recursive=True):\n    dialect = get_dialect_from_path(wrd_file)\n    phonetic_words = get_phonetic_words(wrd_file)\n    if phonetic_words:\n        for item in phonetic_words:\n            word = item[\"word\"].lower()\n            phones = tuple(item[\"phones\"])\n            word_pronunciations_detailed[word].append({\n                'phones': phones,\n                'dialect': dialect,\n                'file': wrd_file\n            })\n\n# Count by dialect\ndialect_counts = Counter()\nfor word, prons in word_pronunciations_detailed.items():\n    for p in prons:\n        dialect_counts[p['dialect']] += 1\n\nprint(\"Pronunciations by dialect region:\")\nfor dr, name in DIALECT_REGIONS.items():\n    print(f\"  {dr} ({name}): {dialect_counts.get(dr, 0)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Context-sensitive transformation extraction with dialect tracking\nfrom collections import defaultdict\n\n# Data structures for context-sensitive rules\n# Key: (phone, context_str) -> Counter of transformations\ncontext_substitutions = defaultdict(Counter)  # (ref_phone, context) -> {actual_phone: count}\ncontext_deletions = defaultdict(Counter)       # (ref_phone, context) -> count\ncontext_matches = defaultdict(Counter)         # (ref_phone, context) -> count\n\n# Dialect-specific tracking\n# Key: dialect -> (phone, context) -> Counter\ndialect_substitutions = defaultdict(lambda: defaultdict(Counter))\ndialect_deletions = defaultdict(lambda: defaultdict(Counter))\ndialect_matches = defaultdict(lambda: defaultdict(Counter))\n\ndef align_with_positions(ref, hyp):\n    \"\"\"\n    Align sequences and return operations with reference positions.\n    Returns list of (op, ref_idx, ref_phone, hyp_phone)\n    \"\"\"\n    m, n = len(ref), len(hyp)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if ref[i-1] == hyp[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n    \n    ops = []\n    i, j = m, n\n    while i > 0 or j > 0:\n        if i > 0 and j > 0 and ref[i-1] == hyp[j-1]:\n            ops.append(('match', i-1, ref[i-1], hyp[j-1]))\n            i -= 1\n            j -= 1\n        elif i > 0 and j > 0 and dp[i][j] == dp[i-1][j-1] + 1:\n            ops.append(('sub', i-1, ref[i-1], hyp[j-1]))\n            i -= 1\n            j -= 1\n        elif i > 0 and dp[i][j] == dp[i-1][j] + 1:\n            ops.append(('del', i-1, ref[i-1], None))\n            i -= 1\n        else:\n            ops.append(('ins', None, None, hyp[j-1]))\n            j -= 1\n    \n    return list(reversed(ops))\n\n# Process all words with context and dialect info\nif timit_dict:\n    for word, pron_list in word_pronunciations_detailed.items():\n        if word not in timit_dict:\n            continue\n        \n        dict_pron = timit_dict[word]\n        \n        for pron_info in pron_list:\n            actual_pron = pron_info['phones']\n            dialect = pron_info['dialect']\n            \n            alignment = align_with_positions(dict_pron, actual_pron)\n            \n            for op, ref_idx, ref_phone, actual_phone in alignment:\n                if ref_idx is None:  # insertion - no reference context\n                    continue\n                \n                # Get context from reference pronunciation\n                left, right = get_context(dict_pron, ref_idx)\n                ctx = context_to_str(left, right)\n                \n                if op == 'match':\n                    context_matches[(ref_phone, ctx)][ref_phone] += 1\n                    dialect_matches[dialect][(ref_phone, ctx)][ref_phone] += 1\n                elif op == 'sub':\n                    context_substitutions[(ref_phone, ctx)][actual_phone] += 1\n                    dialect_substitutions[dialect][(ref_phone, ctx)][actual_phone] += 1\n                elif op == 'del':\n                    context_deletions[(ref_phone, ctx)]['DEL'] += 1\n                    dialect_deletions[dialect][(ref_phone, ctx)]['DEL'] += 1\n\n    print(f\"Context-sensitive analysis complete\")\n    print(f\"Unique (phone, context) pairs with substitutions: {len(context_substitutions)}\")\n    print(f\"Unique (phone, context) pairs with deletions: {len(context_deletions)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display context-sensitive rules\ndef compute_context_rules(context_matches, context_substitutions, context_deletions, \n                          min_count=5, min_rate=2.0):\n    \"\"\"Compute context-sensitive transformation rules\"\"\"\n    rules = {}\n    \n    # Collect all (phone, context) pairs\n    all_pairs = set(context_matches.keys())\n    all_pairs.update(context_substitutions.keys())\n    all_pairs.update(context_deletions.keys())\n    \n    for phone, ctx in all_pairs:\n        # Total occurrences in this context\n        total = sum(context_matches.get((phone, ctx), Counter()).values())\n        total += sum(context_substitutions.get((phone, ctx), Counter()).values())\n        total += sum(context_deletions.get((phone, ctx), Counter()).values())\n        \n        if total < min_count:\n            continue\n        \n        transformations = []\n        \n        # Substitutions\n        for target, count in context_substitutions.get((phone, ctx), Counter()).items():\n            if count >= min_count:\n                rate = count / total * 100\n                if rate >= min_rate:\n                    transformations.append(('sub', target, count, rate))\n        \n        # Deletions\n        del_count = sum(context_deletions.get((phone, ctx), Counter()).values())\n        if del_count >= min_count:\n            rate = del_count / total * 100\n            if rate >= min_rate:\n                transformations.append(('del', None, del_count, rate))\n        \n        if transformations:\n            transformations.sort(key=lambda x: -x[2])\n            rules[(phone, ctx)] = {\n                'total': total,\n                'transforms': transformations\n            }\n    \n    return rules\n\nctx_rules = compute_context_rules(context_matches, context_substitutions, context_deletions)\n\n# Display organized by phone\nprint(\"=== Context-Sensitive Rules (min 5 occurrences, min 2% rate) ===\\n\")\n\n# Group by phone\nby_phone = defaultdict(list)\nfor (phone, ctx), data in ctx_rules.items():\n    by_phone[phone].append((ctx, data))\n\nfor phone in sorted(by_phone.keys()):\n    print(f\"\\n{phone}:\")\n    for ctx, data in sorted(by_phone[phone], key=lambda x: -x[1]['total']):\n        print(f\"  in context {ctx} (n={data['total']}):\")\n        for op, target, count, rate in data['transforms']:\n            if op == 'del':\n                print(f\"    -> ∅ (delete): {count} ({rate:.1f}%)\")\n            else:\n                print(f\"    -> {target}: {count} ({rate:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analyze dialect-specific transformation patterns\ndef compute_dialect_rules(dialect, min_count=3, min_rate=2.0):\n    \"\"\"Compute rules for a specific dialect\"\"\"\n    d_matches = dialect_matches[dialect]\n    d_subs = dialect_substitutions[dialect]\n    d_dels = dialect_deletions[dialect]\n    \n    return compute_context_rules(d_matches, d_subs, d_dels, min_count, min_rate)\n\n# Compare transformation rates across dialects for interesting phones\ninteresting_transforms = [\n    ('t', 'V_V'),   # t-flapping between vowels\n    ('d', 'V_V'),   # d-flapping\n    ('r', 'V_#'),   # r-dropping word-finally after vowel\n    ('ih', 'C_C'),  # vowel reduction\n    ('ae', 'C_C'),  # vowel shifts\n]\n\nprint(\"=== Dialect Comparison for Select Transformations ===\\n\")\n\nfor phone, ctx in interesting_transforms:\n    print(f\"\\n{phone} in context {ctx}:\")\n    \n    for dr in sorted(DIALECT_REGIONS.keys()):\n        d_matches = dialect_matches[dr]\n        d_subs = dialect_substitutions[dr]\n        d_dels = dialect_deletions[dr]\n        \n        total = sum(d_matches.get((phone, ctx), Counter()).values())\n        total += sum(d_subs.get((phone, ctx), Counter()).values())\n        total += sum(d_dels.get((phone, ctx), Counter()).values())\n        \n        if total < 5:\n            continue\n        \n        # Get top transformations\n        transforms = []\n        for target, count in d_subs.get((phone, ctx), Counter()).most_common(3):\n            rate = count / total * 100\n            if rate >= 1.0:\n                transforms.append(f\"{target}:{rate:.0f}%\")\n        \n        del_count = sum(d_dels.get((phone, ctx), Counter()).values())\n        if del_count > 0:\n            rate = del_count / total * 100\n            if rate >= 1.0:\n                transforms.append(f\"∅:{rate:.0f}%\")\n        \n        if transforms:\n            print(f\"  {dr} ({DIALECT_REGIONS[dr]}): n={total}, {', '.join(transforms)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Syllabic consonant analysis\n# Syllabics (em, en, el, eng) typically arise from vowel + consonant -> syllabic\n# e.g., \"button\" /b ah t ax n/ -> [b ah q en] (schwa + n -> syllabic n)\n\nSYLLABIC_MAPPINGS = {\n    'em': ('ax', 'm'),  # schwa + m -> syllabic m\n    'en': ('ax', 'n'),  # schwa + n -> syllabic n  \n    'el': ('ax', 'l'),  # schwa + l -> syllabic l\n    'eng': ('ax', 'ng'), # schwa + ng -> syllabic ng\n    'axr': ('ax', 'r'),  # can also be er -> axr\n}\n\n# Track syllabic realizations\nsyllabic_patterns = defaultdict(Counter)\n\nif timit_dict:\n    for word, pron_list in word_pronunciations_detailed.items():\n        if word not in timit_dict:\n            continue\n        \n        dict_pron = timit_dict[word]\n        \n        for pron_info in pron_list:\n            actual_pron = pron_info['phones']\n            \n            # Look for syllabics in actual pronunciation\n            for i, phone in enumerate(actual_pron):\n                if phone in SYLLABIC_MAPPINGS:\n                    # What was this in the dictionary?\n                    # Try to find the corresponding position\n                    alignment = align_with_positions(dict_pron, actual_pron)\n                    \n                    # Find what aligned to this syllabic\n                    dict_phones_at_pos = []\n                    for op, ref_idx, ref_phone, hyp_phone in alignment:\n                        if hyp_phone == phone:\n                            if ref_phone:\n                                dict_phones_at_pos.append(ref_phone)\n                    \n                    if dict_phones_at_pos:\n                        pattern = tuple(dict_phones_at_pos)\n                        syllabic_patterns[phone][pattern] += 1\n\nprint(\"=== Syllabic Consonant Patterns ===\")\nprint(\"(What dictionary sequences become syllabic consonants)\\n\")\n\nfor syllabic in sorted(syllabic_patterns.keys()):\n    print(f\"\\n{syllabic} (syllabic {syllabic[-1] if syllabic != 'axr' else 'r'}):\")\n    for pattern, count in syllabic_patterns[syllabic].most_common(10):\n        print(f\"  {' + '.join(pattern)} -> {syllabic}: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Export comprehensive rules to JSON\nimport json\n\ndef convert_rules_to_arpabet_format(ctx_rules, syllabic_patterns):\n    \"\"\"Convert all rules to ARPABET format for CMUdict application\"\"\"\n    \n    export = {\n        'context_rules': {},\n        'syllabic_rules': {},\n        'dialect_weights': {}\n    }\n    \n    # Context-sensitive rules\n    for (phone, ctx), data in ctx_rules.items():\n        src = TIMIT_TO_ARPABET.get(phone, phone.upper())\n        if not src:\n            continue\n        \n        key = f\"{src}/{ctx}\"\n        transforms = []\n        for op, target, count, rate in data['transforms']:\n            if op == 'del':\n                transforms.append({\n                    'target': None,\n                    'count': count,\n                    'rate': round(rate, 2)\n                })\n            else:\n                tgt = TIMIT_TO_ARPABET.get(target, target.upper())\n                if tgt and tgt != src:\n                    transforms.append({\n                        'target': tgt,\n                        'count': count,\n                        'rate': round(rate, 2)\n                    })\n        \n        if transforms:\n            export['context_rules'][key] = {\n                'total': data['total'],\n                'transforms': transforms\n            }\n    \n    # Syllabic rules (sequences that become syllabic consonants)\n    for syllabic, patterns in syllabic_patterns.items():\n        tgt = TIMIT_TO_ARPABET.get(syllabic, syllabic.upper())\n        syllabic_key = f\"syllabic_{tgt}\"\n        export['syllabic_rules'][syllabic_key] = []\n        \n        for pattern, count in patterns.most_common(5):\n            src_pattern = [TIMIT_TO_ARPABET.get(p, p.upper()) for p in pattern]\n            src_pattern = [p for p in src_pattern if p]  # filter empties\n            if src_pattern:\n                export['syllabic_rules'][syllabic_key].append({\n                    'source': src_pattern,\n                    'count': count\n                })\n    \n    # Dialect weights (relative frequency of transformations by dialect)\n    for dr in DIALECT_REGIONS:\n        dialect_rules = compute_dialect_rules(dr, min_count=3, min_rate=1.0)\n        if dialect_rules:\n            export['dialect_weights'][dr] = {\n                'name': DIALECT_REGIONS[dr],\n                'rule_count': len(dialect_rules)\n            }\n    \n    return export\n\ncomprehensive_rules = convert_rules_to_arpabet_format(ctx_rules, syllabic_patterns)\n\nwith open(\"timit_comprehensive_rules.json\", \"w\") as f:\n    json.dump(comprehensive_rules, f, indent=2)\n\nprint(f\"Exported comprehensive rules:\")\nprint(f\"  Context-sensitive rules: {len(comprehensive_rules['context_rules'])}\")\nprint(f\"  Syllabic patterns: {len(comprehensive_rules['syllabic_rules'])}\")\nprint(f\"  Dialect regions: {len(comprehensive_rules['dialect_weights'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Apply context-sensitive rules to generate pronunciation variants\ndef generate_context_variants(pronunciation, rules, max_variants=20):\n    \"\"\"\n    Generate pronunciation variants using context-sensitive rules.\n    \"\"\"\n    # Define ARPABET phone classes\n    ARPABET_VOWELS = {'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', \n                      'IH', 'IY', 'OW', 'OY', 'UH', 'UW'}\n    \n    def get_arpabet_context(pron, idx):\n        \"\"\"Get V/C/# context for ARPABET phone\"\"\"\n        def classify(phone):\n            base = ''.join(c for c in phone if not c.isdigit())\n            if base in ARPABET_VOWELS:\n                return 'V'\n            return 'C'\n        \n        left = classify(pron[idx-1]) if idx > 0 else '#'\n        right = classify(pron[idx+1]) if idx < len(pron)-1 else '#'\n        return f\"{left}_{right}\"\n    \n    variants = set()\n    variants.add(tuple(pronunciation))\n    \n    for i, phone in enumerate(pronunciation):\n        phone_base = ''.join(c for c in phone if not c.isdigit())\n        stress = ''.join(c for c in phone if c.isdigit())\n        ctx = get_arpabet_context(pronunciation, i)\n        \n        rule_key = f\"{phone_base}/{ctx}\"\n        \n        if rule_key in rules['context_rules']:\n            rule_data = rules['context_rules'][rule_key]\n            for transform in rule_data['transforms']:\n                target = transform['target']\n                new_pron = list(pronunciation)\n                \n                if target is None:\n                    # Deletion\n                    new_pron = new_pron[:i] + new_pron[i+1:]\n                else:\n                    # Substitution - preserve stress\n                    new_pron[i] = target + stress\n                \n                variants.add(tuple(new_pron))\n                \n                if len(variants) >= max_variants:\n                    return list(variants)\n    \n    return list(variants)\n\n# Example: \"butter\" with context-sensitive t-flapping\nexample_pron = ['B', 'AH1', 'T', 'ER0']\nprint(f\"Base pronunciation: {' '.join(example_pron)}\")\nprint(\"\\nContext-sensitive variants:\")\nfor var in generate_context_variants(example_pron, comprehensive_rules):\n    print(f\"  {' '.join(var)}\")\n\n# Example: \"button\" with syllabic n\nexample_pron2 = ['B', 'AH1', 'T', 'AH0', 'N']\nprint(f\"\\nBase pronunciation: {' '.join(example_pron2)}\")\nprint(\"Context-sensitive variants:\")\nfor var in generate_context_variants(example_pron2, comprehensive_rules):\n    print(f\"  {' '.join(var)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display most common transformations\nprint(\"=== Top 20 Substitutions ===\")\nfor (ref, actual), count in substitutions.most_common(20):\n    # Compute rate: how often this phone gets this substitution vs staying the same\n    total_occurrences = matches[ref] + sum(c for (r, _), c in substitutions.items() if r == ref)\n    rate = count / total_occurrences * 100 if total_occurrences > 0 else 0\n    print(f\"  {ref} -> {actual}: {count} ({rate:.1f}%)\")\n\nprint(\"\\n=== Top 20 Deletions ===\")\nfor phone, count in deletions.most_common(20):\n    total_occurrences = matches[phone] + deletions[phone] + sum(c for (r, _), c in substitutions.items() if r == phone)\n    rate = count / total_occurrences * 100 if total_occurrences > 0 else 0\n    print(f\"  {phone} deleted: {count} ({rate:.1f}%)\")\n\nprint(\"\\n=== Top 20 Insertions ===\")\nfor phone, count in insertions.most_common(20):\n    print(f\"  {phone} inserted: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Build transformation rules with probabilities\n# Format suitable for applying to CMUdict\n\ndef compute_transformation_rules(matches, substitutions, deletions, min_count=5, min_rate=1.0):\n    \"\"\"\n    Compute transformation rules from the collected statistics.\n    Returns dict: phone -> list of (target, probability) where target can be a phone or None (deletion)\n    \"\"\"\n    rules = {}\n    \n    # Get all phones that appear in the reference\n    all_ref_phones = set(matches.keys())\n    all_ref_phones.update(r for r, _ in substitutions.keys())\n    all_ref_phones.update(deletions.keys())\n    \n    for phone in all_ref_phones:\n        # Total occurrences of this phone in reference\n        total = matches[phone]\n        total += deletions.get(phone, 0)\n        total += sum(c for (r, _), c in substitutions.items() if r == phone)\n        \n        if total == 0:\n            continue\n        \n        transformations = []\n        \n        # Add substitutions\n        for (ref, actual), count in substitutions.items():\n            if ref == phone and count >= min_count:\n                rate = count / total * 100\n                if rate >= min_rate:\n                    transformations.append((actual, count, rate))\n        \n        # Add deletions\n        del_count = deletions.get(phone, 0)\n        if del_count >= min_count:\n            rate = del_count / total * 100\n            if rate >= min_rate:\n                transformations.append((None, del_count, rate))\n        \n        if transformations:\n            # Sort by count descending\n            transformations.sort(key=lambda x: -x[1])\n            rules[phone] = transformations\n    \n    return rules\n\nrules = compute_transformation_rules(matches, substitutions, deletions)\n\nprint(\"=== Transformation Rules (min 5 occurrences, min 1% rate) ===\")\nfor phone, transforms in sorted(rules.items()):\n    print(f\"\\n{phone}:\")\n    for target, count, rate in transforms:\n        if target is None:\n            print(f\"  -> ∅ (delete): {count} ({rate:.1f}%)\")\n        else:\n            print(f\"  -> {target}: {count} ({rate:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# TIMIT to ARPABET (CMUdict) phone mapping\n# TIMIT uses a slightly different phoneset than CMUdict\nTIMIT_TO_ARPABET = {\n    # Vowels - TIMIT often has more distinctions\n    'ax': 'AH',      # schwa\n    'ix': 'IH',      # reduced high front (often schwa-like)\n    'ux': 'UW',      # reduced high back\n    'axr': 'ER',     # schwa + r\n    'ax-h': 'AH',    # breathy schwa\n    'em': 'M',       # syllabic m (CMU doesn't have this)\n    'en': 'N',       # syllabic n (CMU doesn't have this)  \n    'eng': 'NG',     # syllabic ng\n    'el': 'L',       # syllabic l (CMU doesn't have this)\n    'nx': 'N',       # flap (alveolar nasal)\n    'dx': 'D',       # flap (often realized as D or T)\n    'q': '',         # glottal stop (not in CMU)\n    'hv': 'HH',      # voiced h\n    # Direct mappings (lowercase to uppercase)\n    'aa': 'AA', 'ae': 'AE', 'ah': 'AH', 'ao': 'AO', 'aw': 'AW',\n    'ay': 'AY', 'eh': 'EH', 'er': 'ER', 'ey': 'EY', 'ih': 'IH',\n    'iy': 'IY', 'ow': 'OW', 'oy': 'OY', 'uh': 'UH', 'uw': 'UW',\n    'b': 'B', 'ch': 'CH', 'd': 'D', 'dh': 'DH', 'f': 'F', 'g': 'G',\n    'hh': 'HH', 'jh': 'JH', 'k': 'K', 'l': 'L', 'm': 'M', 'n': 'N',\n    'ng': 'NG', 'p': 'P', 'r': 'R', 's': 'S', 'sh': 'SH', 't': 'T',\n    'th': 'TH', 'v': 'V', 'w': 'W', 'y': 'Y', 'z': 'Z', 'zh': 'ZH',\n}\n\ndef timit_to_arpabet(timit_phones):\n    \"\"\"Convert TIMIT phone sequence to ARPABET (CMUdict format)\"\"\"\n    result = []\n    for phone in timit_phones:\n        mapped = TIMIT_TO_ARPABET.get(phone, phone.upper())\n        if mapped:  # Skip empty mappings (like glottal stop)\n            result.append(mapped)\n    return tuple(result)\n\n# Convert rules to ARPABET\narpabet_rules = {}\nfor phone, transforms in rules.items():\n    src = TIMIT_TO_ARPABET.get(phone, phone.upper())\n    if not src:\n        continue\n    if src not in arpabet_rules:\n        arpabet_rules[src] = []\n    for target, count, rate in transforms:\n        if target is None:\n            arpabet_rules[src].append((None, count, rate))\n        else:\n            tgt = TIMIT_TO_ARPABET.get(target, target.upper())\n            if tgt and tgt != src:  # Don't add identity mappings\n                arpabet_rules[src].append((tgt, count, rate))\n\nprint(\"=== Rules in ARPABET format ===\")\nfor phone, transforms in sorted(arpabet_rules.items()):\n    if transforms:\n        print(f\"\\n{phone}:\")\n        for target, count, rate in transforms:\n            if target is None:\n                print(f\"  -> ∅ (delete): {count} ({rate:.1f}%)\")\n            else:\n                print(f\"  -> {target}: {count} ({rate:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Export rules to JSON for later use\nimport json\n\nexport_rules = {}\nfor phone, transforms in arpabet_rules.items():\n    if transforms:\n        export_rules[phone] = [\n            {\"target\": t, \"count\": c, \"rate\": round(r, 2)} \n            for t, c, r in transforms\n        ]\n\nwith open(\"timit_transformation_rules.json\", \"w\") as f:\n    json.dump(export_rules, f, indent=2)\n    \nprint(f\"Exported {len(export_rules)} phone rules to timit_transformation_rules.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example: Generate pronunciation variants for a CMUdict entry\ndef generate_variants(pronunciation, rules, max_variants=10):\n    \"\"\"\n    Generate pronunciation variants by applying transformation rules.\n    Uses a simple approach: apply one rule at a time to generate variants.\n    \"\"\"\n    variants = set()\n    variants.add(tuple(pronunciation))\n    \n    for i, phone in enumerate(pronunciation):\n        # Strip stress markers for lookup\n        phone_base = ''.join(c for c in phone if not c.isdigit())\n        \n        if phone_base in rules:\n            for rule in rules[phone_base]:\n                target = rule[\"target\"]\n                # Create variant\n                new_pron = list(pronunciation)\n                if target is None:\n                    # Deletion\n                    new_pron = new_pron[:i] + new_pron[i+1:]\n                else:\n                    # Preserve stress marker if present\n                    stress = ''.join(c for c in phone if c.isdigit())\n                    new_pron[i] = target + stress\n                variants.add(tuple(new_pron))\n                \n                if len(variants) >= max_variants:\n                    break\n        \n        if len(variants) >= max_variants:\n            break\n    \n    return list(variants)\n\n# Example with a word\nexample_pron = ['W', 'AO1', 'T', 'ER0']  # \"water\" in CMUdict format\nprint(f\"Base pronunciation: {' '.join(example_pron)}\")\nprint(\"Variants:\")\nfor var in generate_variants(example_pron, export_rules):\n    print(f\"  {' '.join(var)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}