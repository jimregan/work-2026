---
title: "Papers, 2026"
bibliography: refs.bib
---

###Â [Dependency Parsing Evaluation for Low-resource Spontaneous Speech](https://aclanthology.org/2021.adaptnlp-1.16/) [@liu-prudhommeaux-2021-dependency]

- CHILDES
- Converted to UD (conversion is weird)

### [Alignment of Speech to Highly Imperfect Text Transcriptions](https://ieeexplore.ieee.org/document/4284627)
- [arXiv](https://arxiv.org/abs/cs/0612139)
- phoneme-based alignment
- vowel phonemes detected using a Matlab toolkit for formant estimation
- formants matched to expected values using Euclidean distance
- fricatives detected using spectral energy distribution
- phoneme sequences aligned using dynamic programming edit-distance
- uses [this toolkit](https://www.clear.rice.edu/elec431/projects96/digitalbb/)

```{python}
import numpy as np
import librosa
from scipy import signal

def extract_formants_librosa(audio, sr, n_formants=3):
    """
    Extract formants using librosa's LPC implementation
    
    Args:
        audio: audio frame
        sr: sampling rate
        n_formants: number of formants to extract
    
    Returns:
        formants: array of formant frequencies
    """
    # LPC order rule of thumb
    order = int(2 + sr/1000)
    
    # Get LPC coefficients
    a = librosa.lpc(audio, order=order)
    
    # Find roots of LPC polynomial
    roots = np.roots(a)
    roots = roots[np.imag(roots) >= 0]  # Keep positive frequencies only
    
    # Convert to Hz
    angles = np.angle(roots)
    freqs = np.sort(angles * (sr / (2 * np.pi)))
    
    # Return first n formants
    return freqs[:n_formants]

def detect_vowel_librosa(y, sr, frame_length=0.03, hop_length=0.01):
    """
    Detect vowels in audio signal
    
    Args:
        y: audio signal
        sr: sampling rate
        frame_length: analysis window in seconds
        hop_length: hop between frames in seconds
    
    Returns:
        times: timestamps for each detection
        vowels: detected vowel at each timestamp
        formants: formant values at each timestamp
    """
    # Vowel formant reference (weighted matching)
    vowel_table = {
        'IY': [255, 2330, 3000],   # "heed"
        'IH': [350, 1975, 2560],   # "hid"
        'EH': [560, 1875, 2550],   # "head"
        'AE': [735, 1625, 2465],   # "had"
        'AA': [760, 1065, 2550],   # "hod"
        'AO': [610, 865, 2540],    # "hawed"
        'UW': [290, 940, 2180],    # "who'd"
        'UH': [475, 1070, 2410],   # "hood"
        'AH': [640, 1250, 2610],   # "bud"
    }
    weights = np.array([2, 1, 1])
    
    # Convert to samples
    frame_len_samples = int(frame_length * sr)
    hop_len_samples = int(hop_length * sr)
    
    times = []
    vowels = []
    formant_list = []
    
    # Frame-by-frame analysis
    for i in range(0, len(y) - frame_len_samples, hop_len_samples):
        frame = y[i:i + frame_len_samples]
        
        # Pre-emphasis filter (optional, helps with formant detection)
        frame = librosa.effects.preemphasis(frame)
        
        # Normalize
        frame = frame - np.mean(frame)
        if np.max(np.abs(frame)) > 0:
            frame = frame / np.max(np.abs(frame))
        
        # Extract formants
        try:
            fmnts = extract_formants_librosa(frame, sr)
            
            # Match to closest vowel
            distances = {}
            for vowel, target in vowel_table.items():
                target = np.array(target)
                dist = np.linalg.norm(weights * (target - fmnts))
                distances[vowel] = dist
            
            best_vowel = min(distances, key=distances.get)
            
            times.append(i / sr)
            vowels.append(best_vowel)
            formant_list.append(fmnts)
            
        except:
            # Skip frames where formant detection fails
            continue
    
    return np.array(times), vowels, formant_list

def detect_fricatives_librosa(y, sr, frame_length=0.03, hop_length=0.01):
    """
    Detect fricatives (S, SH) using spectral energy distribution
    
    Args:
        y: audio signal
        sr: sampling rate
        frame_length: analysis window in seconds
        hop_length: hop between frames in seconds
    
    Returns:
        times: timestamps
        fricatives: detected fricative at each timestamp
    """
    # Compute STFT
    n_fft = 2048
    hop_len_samples = int(hop_length * sr)
    
    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_len_samples)
    S = np.abs(D)
    freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)
    
    times = librosa.frames_to_time(range(S.shape[1]), sr=sr, 
                                     hop_length=hop_len_samples)
    fricatives = []
    
    for i in range(S.shape[1]):
        spectrum = S[:, i]
        
        # Energy in frequency bands
        sh_mask = (freqs >= 2500) & (freqs <= 3000)
        s_mask = (freqs >= 3000) & (freqs <= 4000)
        other_mask = (freqs >= 300) & (freqs <= 2500)
        
        sh_energy = np.sum(spectrum[sh_mask])
        s_energy = np.sum(spectrum[s_mask])
        other_energy = np.sum(spectrum[other_mask])
        
        # Normalize
        total = sh_energy + s_energy + other_energy
        if total > 0:
            sh_energy /= total
            s_energy /= total
        
        # Classify
        if s_energy > sh_energy and s_energy > 0.3:
            fricatives.append('S')
        elif sh_energy > s_energy and sh_energy > 0.25:
            fricatives.append('SH')
        else:
            fricatives.append(None)
    
    return times, fricatives

def detect_phonemes(audio_file):
    """
    Complete phoneme detection pipeline
    
    Args:
        audio_file: path to audio file
    
    Returns:
        phoneme_sequence: list of (time, phoneme) tuples
    """
    # Load audio
    y, sr = librosa.load(audio_file, sr=16000)
    
    # Remove silence
    y_trimmed, _ = librosa.effects.trim(y, top_db=20)
    
    # Detect vowels
    vowel_times, vowels, formants = detect_vowel_librosa(y_trimmed, sr)
    
    # Detect fricatives
    fric_times, fricatives = detect_fricatives_librosa(y_trimmed, sr)
    
    # Combine (simple merge by time)
    phoneme_sequence = []
    
    for t, v in zip(vowel_times, vowels):
        phoneme_sequence.append((t, v))
    
    for t, f in zip(fric_times, fricatives):
        if f is not None:
            phoneme_sequence.append((t, f))
    
    # Sort by time
    phoneme_sequence.sort(key=lambda x: x[0])
    
    return phoneme_sequence

# Example usage
if __name__ == "__main__":
    phonemes = detect_phonemes("speech.wav")
    
    print("Time\tPhoneme")
    print("-" * 20)
    for time, phoneme in phonemes[:20]:  # First 20
        print(f"{time:.2f}\t{phoneme}")
```